# Agentic-RAG-Powered-AI-Research-Assistant

ğŸ“š Agentic RAG Powered AI Research Assistant
A Retrieval-Augmented Generation (RAG) powered intelligent research assistant with agentic capabilities, allowing you to query multiple documents, retrieve precise answers, and automate reasoning workflows.
Built using LangChain, ChromaDB, and Streamlit.

ğŸš€ Features
âœ… Document Ingestion â€“ Upload PDFs, text files, or scrape web content.
âœ… Vector Store Integration â€“ Uses ChromaDB for semantic search.
âœ… HuggingFace Embeddings â€“ High-quality embeddings for better retrieval.
âœ… Agentic Capabilities â€“ The assistant can plan, decide, and execute multi-step queries.
âœ… Interactive UI â€“ Built with Streamlit.
âœ… Local LLM Support â€“ Works with Ollama / LLaMA / Mistral locally.

ğŸ› ï¸ Tech Stack
Python 3.10+

LangChain â€“ Retrieval & agent orchestration

ChromaDB â€“ Vector database

HuggingFace Transformers â€“ Embeddings & models

Ollama / LLaMA â€“ Local model serving

Streamlit â€“ Web-based UI

ğŸ“‚ Project Structure
bash
Copy
Edit
Agentic-RAG-Powered-AI-Research-Assistant/
â”‚
â”œâ”€â”€ app.py                      # Streamlit UI entry point
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ vector_store.py          # ChromaDB vector store logic
â”‚   â”œâ”€â”€ document_loader.py       # Document ingestion & preprocessing
â”‚   â”œâ”€â”€ rag_chain.py             # RAG pipeline creation
â”‚   â””â”€â”€ agents.py                 # Agentic reasoning logic
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ data/                        # Sample documents


# 1ï¸âƒ£ Clone the repo
git clone https://github.com/MansiDakhale/Agentic-RAG-Powered-AI-Research-Assistant.git
cd Agentic-RAG-Powered-AI-Research-Assistant

# 2ï¸âƒ£ Create and activate virtual environment
python -m venv A_venv
source A_venv/bin/activate   # Mac/Linux
A_venv\Scripts\activate      # Windows

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ (Optional) Install Ollama for local LLM
# https://ollama.ai/download


# Run the Streamlit app
streamlit run app.py

Step 1: Upload your documents in the sidebar.

Step 2: Ask a research question.

Step 3: The assistant retrieves relevant chunks and responds with context.

ğŸ§  How It Works
Document Ingestion â†’ Files are split into chunks using RecursiveCharacterTextSplitter.

Vectorization â†’ Embeddings generated using HuggingFace models.

Vector Store â†’ Stored in ChromaDB for semantic search.

Retrieval-Augmented Generation â†’ Retrieved chunks + user query â†’ passed to LLM.

Agentic Reasoning â†’ For complex queries, the assistant plans multi-step reasoning using LangChain Agents.

ğŸ“¸ Demo Screenshot:
![alt text](image.png)


ğŸ“Œ Roadmap
 Add support for multi-modal inputs (images + text).

 Implement memory for persistent conversations.

 Integrate OpenAI GPT-4 for cloud inference.

 Deploy to HuggingFace Spaces / Streamlit Cloud.

ğŸ¤ Contributing
Pull requests are welcome! Please fork the repo and submit a PR.

ğŸ“œ License
This project is licensed under the MIT License.